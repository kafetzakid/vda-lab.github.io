<h2 id="word-count-and-beyond">1. Word count and beyond</h2>

<h3 id="word-count-using-hadoop-streaming">1.1 Word count using Hadoop Streaming</h3>

<p>Implement the Python <code>mapper</code> and <code>reducer</code> scripts on the slides of week 6. Run the Hadoop streaming API on the Amazon server. Use the data in ‘AMM_det_H.csv’ from the drugs dataset for this.</p>

<p>Mapper:</p>

<p>```python
#!/usr/bin/env python</p>

<h1 id="simple-mapper-just-like-on-the-lecture-slides">Simple mapper, just like on the lecture slides</h1>
<p># Lines are split on spaces:
#  - good for word count
#  - not good for CSV or TSV files</p>

<p>import sys</p>

<p>for line in sys.stdin:
    line = line.strip()
    words = line.split()
    for word in words:
        print ‘%s\t%s’ % (word, 1)
```</p>

<p>Reducer:</p>

<p>```python
#!/usr/bin/env python</p>

<h1 id="a-simple-reducer-as-used-in-the-lecture">A simple reducer, as used in the lecture</h1>
<p># This reducer can be used for:
# - mapper.py: simple word count
# - mapper1.py: all columns are passed
# - mapper2.py: only active substances are passed</p>

<p>from operator import itemgetter
import sys</p>

<p>current_word = None
current_count = 0
word = None</p>

<p>for line in sys.stdin:
    line = line.strip()
    word, count = line.split(‘\t’, 1)</p>

<pre><code>try:
    count = int(count)
except ValueError:
    continue

# this IF-switch only works because Hadoop sorts map output
# by key (here: word) before it is passed to the reducer
if current_word == word:
    current_count += count
else:
    if current_word:
        # write result to STDOUT
        print '%s\t%s' % (current_word, current_count)
    current_count = count
    current_word = word
</code></pre>

<h1 id="do-not-forget-to-output-the-last-word-if-needed">do not forget to output the last word if needed!</h1>
<p>if current_word == word:
    print ‘%s\t%s’ % (current_word, current_count)
```</p>

<p>Make sure the scripts can be executed (<code>chmod u+x *.py</code> if needed).</p>

<p>To run this example in the shell (not Hadoop!)</p>

<p><code>bash
cat /mnt/bioinformatics_leuven/i0u19a/data/drugdb/AMM_det_H.csv | ./mapper.py | sort -k 1,1 | ./reducer.py
</code></p>

<p>We first have to set some environment variables in order for the rest to work properly:</p>

<p><code>bash
export PATH="$PATH:/mnt/bioinformatics_leuven/homes/tverbeiren/hadoop-common/hadoop-dist/target/hadoop-2.4.0/bin"
export JAVA_HOME="/usr/lib/jvm/java-7-openjdk-amd64/"
export PATH="$PATH:/opt/hadoop/hadoop-dist/target/hadoop-2.4.0/bin"
export PATH="$PATH:/opt/spark/bin"
export SPARK_HOME="/opt/spark"
</code></p>

<p>To run the example using Hadoop, use the following:</p>

<p><code>bash
hadoop jar /opt/hadoop/hadoop-tools/hadoop-streaming/target/hadoop-streaming-2.4.0.jar \
  -mapper mapper.py \
  -reducer reducer.py \
  -input /mnt/bioinformatics_leuven/i0u19a/data/drugdb/AMM_det_H.csv \
  -output output
</code></p>

<p>Does this work? Does it give some insight? Why (not)?</p>

<h3 id="attempt-2">1.2 Attempt 2</h3>

<p>What about splitting the lines (in the mapper) on ‘,’ instead of spaces? Adapt the <code>mapper.py</code> script. Name it <code>mapper1.py</code>. Do we have to change the reducer script?</p>

<p>Mapper:</p>

<p>```python
#!/usr/bin/env python</p>

<h1 id="lines-are-split-on-">Lines are split on ‘,’:</h1>
<p>#  - good for CSV files
# ‘All’ columns are passed to reduce phase</p>

<p>import sys</p>

<p>for line in sys.stdin:
    line = line.strip()
    words = line.split(“,”)
    for word in words:
        print ‘%s\t%s’ % (word, 1)
```</p>

<p>The reducer is the same.</p>

<p>Hadoop can be initiated like this:</p>

<p><code>bash
hadoop jar /opt/hadoop/hadoop-tools/hadoop-streaming/target/hadoop-streaming-2.4.0.jar \
  -mapper mapper1.py \
  -reducer reducer.py \
  -input /mnt/bioinformatics_leuven/i0u19a/data/drugdb/AMM_det_H.csv \
  -output output
</code></p>

<p>Does this work? Does it give some insight? Why (not)? How many times is PARACETAMOL the active substance?</p>

<p>In order to find Paracetamol: Look in the dataset (using for instance <code>less</code>) and find the word. The answer is 136.</p>

<p>What is the most used substance?</p>

<p>Please note that in order to effectively sort in a tab-delimited dataset, you need to the following:</p>

<pre><code>cat  part-00000  | sort -r -g -k2,2 -t$'\t' | head
</code></pre>

<p>Otherwise, spaces are used as delimiters.</p>

<h3 id="finding-out-about-active-substances-directly">1.3 Finding out about active substances directly</h3>

<p>We go one step further. In the previous exercise, we did a word count on all columns in the original data. We now select one specific <em>column</em>.</p>

<p>Rewrite the mapper script to only send active substance names to the reducer. Call the resulting mapper script <code>mapper2.py</code>.</p>

<p>Mapper:</p>

<p>```python
#!/usr/bin/env python</p>

<h1 id="lines-are-split-on--1">Lines are split on ‘,’:</h1>
<p>#  - good for CSV files
# Only 3d column is passed to reduce phase
#  - good for drugdb (AMM_det_H, active subst. column)</p>

<p>import sys</p>

<p>for line in sys.stdin:
    words = line.strip().split(“,”)
    print ‘%s\t%s’ % (words[2], 1)
```</p>

<p>The reducer remains the same.</p>

<p>To run Hadoop:</p>

<p><code>bash
hadoop jar /opt/hadoop/hadoop-tools/hadoop-streaming/target/hadoop-streaming-2.4.0.jar \
  -mapper mapper2.py \
  -reducer reducer.py \
  -input /mnt/bioinformatics_leuven/i0u19a/data/drugdb/AMM_det_H.csv \
  -output output
</code></p>

<p>The most active substance? This is the top-10:</p>

<pre><code>cat  part-00000  | sort -r -g -k2,2 -t$'\t' | head
</code></pre>

<h3 id="active-substances-and-maximum-doses">1.4 Active substances and maximum doses</h3>

<p>Rewrite <code>mapper</code> and <code>reducer</code> such that not only do we count how many occurrences there are of a certain active substance, but also report the maximum dose.</p>

<p><em>Important</em>: do this is 1 run.</p>

<p>Name the scripts <code>mapper3.py</code> and <code>reducer3.py</code>. Think of the assignments you did in preparation of this week’s exercises.</p>

<p>Mapper:</p>

<p>```python
#!/usr/bin/env python</p>

<h1 id="mapper-for-ammdethcsv">Mapper for AMM_det_H.csv</h1>
<p># Lines are split on ‘,’:
#  - good for CSV files
# 2 columns are passed to reducer:
#  - 3d column: Active Substance
#  - 5th column: Dose, double quotes are removed
# We pass 2 values (aka a list) to the reducer.
# Please note that key and value(s) are separated by ‘\t’</p>

<p>import sys</p>

<p>for line in sys.stdin:
    words = line.strip().split(“,”)
    print ‘%s\t%s,%s’ % (words[2], 1, words[4].replace(‘”’, ‘’))
```</p>

<p>Reducer:</p>

<p>```python
#!/usr/bin/env python</p>

<h1 id="a-simple-reducer-as-used-in-the-lecture-1">A simple reducer, as used in the lecture</h1>
<p># This reducer can be used for:
#   - mapper3.py
# The two values are extracted and the maximum value
# of the dose is passed as an additional column in the output.</p>

<p>import sys</p>

<p>current_word = None
current_count = 0
max_dose = 0
word = None</p>

<p>for line in sys.stdin:
    line = line.strip()
    word, rest = line.split(‘\t’)
    count, dose = rest.split(‘,’)</p>

<pre><code>try:
    count = int(count)
except ValueError:
    continue
try:
    dose = float(dose)
except ValueError:
    continue

# this IF-switch only works because Hadoop sorts map output
# by key (here: word) before it is passed to the reducer
if current_word == word:
    current_count += count
    if max_dose &lt; dose:
        max_dose = dose
else:
    if current_word:
        # write result to STDOUT
        print '%s\t%s\t%s' % (current_word, current_count, max_dose)
    current_count = count
    current_word = word
    max_dose = dose
</code></pre>

<h1 id="do-not-forget-to-output-the-last-word-if-needed-1">do not forget to output the last word if needed!</h1>
<p>if current_word == word:
    print ‘%s\t%s\t%s’ % (current_word, current_count, max_dose)
```</p>

<p>Hadoop run:</p>

<p><code>bash
hadoop jar /opt/hadoop/hadoop-tools/hadoop-streaming/target/hadoop-streaming-2.4.0.jar \
  -mapper mapper3.py \
  -reducer reducer3.py \
  -input /mnt/bioinformatics_leuven/i0u19a/data/drugdb/AMM_det_H.csv \
  -output output
</code></p>

<p>Is this a convenient way to work? What would make your life easier?</p>

<p>This is a poor way to pass additional data between two phases. We could use different approaches: <code>JSON</code>, … Another option would be to use a datamodel and work with objects. These objects then need to be converted to a stream of data.</p>

<h2 id="normalization">2. Normalization</h2>

<h3 id="joins">2.1 Joins</h3>

<p>Refer to the exercises on RDBMs, and look up what kind of questions we posed.</p>

<p>One of them was: <em>Which companies have compounds on the market with more than 10 active substances?</em></p>

<p>Think of how you would approach that?</p>

<h3 id="normalization-or-not">2.2 Normalization or not?</h3>

<p>What is your opinion on normalization? Is it a good thing?</p>

<h3 id="reduce-side-join">2.3 Reduce-side Join</h3>

<p>Rewrite the map and reduce phase such that we can get the answer to the question above. Call the scripts <code>mapperJoin.py</code> and <code>reducerJoin.py</code>.</p>

<p>Mapper:</p>

<p>```
#!/usr/bin/env python</p>

<h1 id="simple-mapper-just-like-on-the-lecture-slides-1">Simple mapper, just like on the lecture slides</h1>
<p># Lines are split on spaces:
#  - good for word count
#  - not good for CSV or TSV files</p>

<p>import sys</p>

<p>abs_path = ‘/mnt/bioinformatics_leuven/i0u19a/data/drugdb/’</p>

<p>amm_det = open(abs_path + ‘AMM_det_H.csv’, ‘r’)
amm = open(abs_path + ‘AMM_H.csv’, ‘r’)</p>

<p>for line in amm:
    words = line.strip().split(‘,’)
    print ‘%s.%s\t%s’ % (words[1], “1”, words[2])</p>

<p>for line in amm_det:
    words = line.strip().split(‘,’)
    print ‘%s.%s\t%s’ % (words[1], “2”, words[2])
```</p>

<p>Reducer:</p>

<p>```
#!/usr/bin/env python</p>

<h1 id="this-reduces-the-special-format-output-for-joining">This reduces the special-format output for joining</h1>
<p># The first entry (.1) is the compound
# The second entry and sometimes next ones are the substances</p>

<p>import sys</p>

<p>current_cti = None
current_name = None
current_count = 0
word = None</p>

<p>for line in sys.stdin:
    line = line.strip()
    keys, value = line.split(‘\t’)
    cti,d = keys.split(‘.’)</p>

<pre><code>if int(d) == 1:
    if current_cti:
        print '%s\t%s\t%s' % (current_cti, current_count, current_value)
    current_cti = cti
    current_count = 0
    current_value = value
if int(d) == 2:
    current_count = current_count + 1
</code></pre>

<h1 id="do-not-forget-to-output-the-last-word-if-needed-2">do not forget to output the last word if needed!</h1>
<p>print ‘%s\t%s\t%s’ % (current_cti, current_count, value)
```</p>

<p>What is the top-10?</p>

<p><code>bash
hadoop jar /opt/hadoop/hadoop-tools/hadoop-streaming/target/hadoop-streaming-2.4.0.jar \
  -mapper mapperJoin.py \
  -reducer reducerJoin.py \
  -input mapper.py \
  -output output
</code></p>

<p>The <code>-input</code> flag is strange and is actually a dummy file as the real input files are defined in the mapper. Be careful, this would not work as-is on HDFS!</p>

<p>To get a sorted listing:</p>

<pre><code>cat  part-00000  | sort -r -g -k2,2 -t$'\t' | head
</code></pre>

<p>One more thing: ask yourself if this approach would work when running on a cluster. Why would this not be a good idea?</p>

<h2 id="spark">3. Spark</h2>

<p>Do the same exercise as above with the drug database, but now using the Spark interactive shell. It can be launched in the following way:</p>

<p><code>
pyspark
</code></p>

<p>Refer to the examples given in the lecture and see how far you can get.</p>

<p>Take a look at: <a href="http://spark.apache.org/docs/latest/programming-guide.html">http://spark.apache.org/docs/latest/programming-guide.html</a></p>

<p>Some examples:</p>

<p><code>python
file = sc.textFile("/mnt/bioinformatics_leuven/i0u19a/data/drugdb/AMM_det_H.csv")
counts = file.flatMap(lambda line: line.split(",")) \
             .map(lambda word: (word, 1)) \
             .reduceByKey(lambda a, b: a + b)
sorted = counts.map(lambda x: (x[1],x[0]) ).sortByKey(False)
</code></p>

<p>Top-10</p>

<pre><code>sorted.take(10)
</code></pre>

<p>Paracetamol:</p>

<pre><code>sorted.filter(lambda x: x[1]=='"PARACETAMOL"').collect()
</code></pre>

<p><code>
result2 = file.map(lambda line: line.split(",")) \
    .map(lambda x: x[2]) \
    .map(lambda word: (word, 1)) \
    .reduceByKey(lambda a, b: a + b)
result2.filter(lambda x: x[0] == '"PARACETAMOL"').collect()
</code></p>

<p>Please remark the two types of quotes.</p>

